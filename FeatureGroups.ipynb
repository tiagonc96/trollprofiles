{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 json files found\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import spacy\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "\n",
    "\n",
    "jsons=[]\n",
    "for filename in glob.iglob(\"D:\\\\socialbus\\\\users\\\\*\" + '*', recursive=True):\n",
    "    try:\n",
    "        jsons.append(pd.read_json(filename,lines=True))\n",
    "        #print(filename)\n",
    "        #print(\"deu\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(str(len(jsons)) + \" json files found\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           username                                               text  \\\n",
      "0       _marianacm_  RT @eujafizaquiloou: Filipinos brancos ou negr...   \n",
      "1        porconeira  RT @celsounz: Matéria imperdível do amigo Robe...   \n",
      "2      CamaCaminhas  RT @selecaoportugal: É dia de estreia da Seleç...   \n",
      "3      tugatripeira            @Anabela66392672 @freirept Um luxo !! 😋   \n",
      "4       Iight_house  RT @aerialsoph: portanto, estávamos a apresent...   \n",
      "5      JoanaBizarro                     @ortoirlandes @FranciscaVaz6 😁   \n",
      "6       rebelomauro                            @KukaGR 12.5 milhões...   \n",
      "7         tecno_boy  É um épico de cinema o Parasite. Todo aquele s...   \n",
      "8          evilashs  @SICNoticias digam a este atrasado mental que ...   \n",
      "9          jony2430  RT @odedonafrida: Toda a gente sabe que em Por...   \n",
      "10         ymoretto           @deeeehapp meu deus que barulhinho fofo!   \n",
      "11  BarreiraCecilia  @BBris BB. O cavalheirismo era uma forma de do...   \n",
      "12    franciscamcs_  RT @duusales: O lado negro do coronavirus. Não...   \n",
      "13         EnCosta4  RT @goncalocvale: Jota também está a ser queim...   \n",
      "14         safilsil  @OArtistaDoDia @Record_Portugal Gestão danosa ...   \n",
      "15  imchicopereira1          @thenwtakeberlin @mdmarley___ Sem dúvida.   \n",
      "16        amandarzz  @ufrgsnoticias e os bixos enq???? https://t.co...   \n",
      "17  catarinascnunes                                     Sobreviventes.   \n",
      "18    augustoscosta  @jfelixcardoso De fonte segura num hospital do...   \n",
      "19        Strompita  @pedrocr75444218 Reminiscências das cábulas à ...   \n",
      "20      Cs45Taxiana  RT @kennyberg666: Os meus beats dizem estas co...   \n",
      "21     nunovalinhas  @bcoelhone @TxigaVira De uma coisa não tenho q...   \n",
      "\n",
      "   time_of_comment  tweetLength  nrStopwords  nrHashtags  nrMentions  nrUrls  \\\n",
      "0         17:19:09           72            1           0           1       0   \n",
      "1         17:20:15          126            1           0           3       1   \n",
      "2         20:43:28          139            3           2           2       0   \n",
      "3         13:46:28           39            0           0           2       0   \n",
      "4         13:55:55          139           10           0           1       0   \n",
      "5         13:56:24           30            0           0           2       0   \n",
      "6         12:51:58           23            0           0           1       0   \n",
      "7         12:53:31           81            6           0           0       0   \n",
      "8         21:40:50          140            9           0           1       1   \n",
      "9         22:36:54          104            7           0           1       0   \n",
      "10        12:52:27           40            2           0           1       0   \n",
      "11        19:59:56          140            9           0           1       1   \n",
      "12        20:37:31          139            6           0           1       0   \n",
      "13        21:55:29          125           10           0           1       0   \n",
      "14        22:45:46           93            3           0           2       0   \n",
      "15        17:59:55           41            0           0           2       0   \n",
      "16        18:05:48           57            2           0           1       0   \n",
      "17        20:45:38           14            0           0           0       0   \n",
      "18        12:59:51          140            6           0           1       1   \n",
      "19        13:59:53           58            2           0           1       0   \n",
      "20        14:59:54           50            2           0           1       0   \n",
      "21        15:04:39          140            8           0           2       1   \n",
      "\n",
      "    nrEmoticons  nrSwears  nrFavs  nrVerbs  nrNouns  nrAdjectives  \n",
      "0           0.0         0       0        0        2             1  \n",
      "1           0.0         0       0        1        4             3  \n",
      "2           0.0         0       0        4        2             2  \n",
      "3           0.0         0       0        1        3             0  \n",
      "4           0.0         0       0        2        3             1  \n",
      "5           0.0         0       0        0        0             0  \n",
      "6           0.0         0       0        0        1             0  \n",
      "7           0.0         0       0        2        5             0  \n",
      "8           NaN         0       0        3        6             1  \n",
      "9           NaN         0       0        2        2             0  \n",
      "10          NaN         0       0        0        1             0  \n",
      "11          NaN         0       0        3        4             1  \n",
      "12          NaN         0       0        3        5             1  \n",
      "13          NaN         0       0        4        2             0  \n",
      "14          NaN         0       0        4        1             2  \n",
      "15          NaN         0       0        0        1             0  \n",
      "16          NaN         0       0        1        2             0  \n",
      "17          NaN         0       0        0        1             0  \n",
      "18          NaN         0       0        1        3             3  \n",
      "19          NaN         0       0        1        1             2  \n",
      "20          NaN         0       0        1        2             0  \n",
      "21          NaN         0       0        3        5             1  \n"
     ]
    }
   ],
   "source": [
    "    i=0\n",
    "    tweets=[]\n",
    "    \n",
    "    for json in jsons:\n",
    "        i=len(json)-1\n",
    "        while(i>1):\n",
    "        \n",
    "            try:\n",
    "          \n",
    "                t1=Tweet(json['user'][i]['screen_name'],json['text'][i],get_time_of_comment(json['created_at'][i]),\n",
    "                  get_stopword_nr(json['text'][i]),get_nr_of_hashtags(json['metadata'][i]),\n",
    "                    get_nr_of_mentions(json['metadata'][i]),get_nr_of_urls(json['metadata'][i]),\n",
    "                  get_nr_of_emoticons(json['metadata'][i]),get_swear_count(json['text'][i]),\n",
    "                  get_favorites_count(json['favorite_count'][i]),get_verb_count(json['text'][i]),\n",
    "                  get_noun_count(json['text'][i]),get_adj_count(json['text'][i]),\n",
    "                  get_tweet_length(json['text'][i]))\n",
    "            \n",
    "                  \n",
    "        \n",
    "                tweets.append(t1)\n",
    "            \n",
    "            except:\n",
    "        \n",
    "                t1=Tweet(json['user'][i]['screen_name'],json['text'][i],get_time_of_comment(json['created_at'][i]),\n",
    "                  get_stopword_nr(json['text'][i]),get_nr_of_hashtags(json['entities'][i]),\n",
    "                  get_nr_of_mentions(json['entities'][i]),get_nr_of_urls(json['entities'][i]),\n",
    "                  get_nr_of_emoticons(json['entities'][i]),get_swear_count(json['text'][i]),\n",
    "                  get_favorites_count(json['favorite_count'][i]),get_verb_count(json['text'][i]),\n",
    "                  get_noun_count(json['text'][i]),get_adj_count(json['text'][i]),\n",
    "                  get_tweet_length(json['text'][i]))\n",
    "                \n",
    "        \n",
    "                tweets.append(t1)\n",
    "            \n",
    "            \n",
    "        \n",
    "            i=i-1 \n",
    "            break\n",
    "  \n",
    "\n",
    "    \n",
    "    #data=pd.Series(tweets)\n",
    "    create_dataFrame(tweets)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_nr(sentence):\n",
    "    \n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    count=0\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    for stopword in stopwords:\n",
    "        for word in word_tokens:\n",
    "            if stopword==word:\n",
    "                count+=1\n",
    "       \n",
    "    \n",
    "    return count\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_of_comment(jsonLine):\n",
    "     \n",
    "    dateStr=str(jsonLine)\n",
    "       \n",
    "    fullDateArray=dateStr.split()\n",
    "    time=fullDateArray[1].split('+')\n",
    "    \n",
    "    return time[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_hashtags(jsonLine):\n",
    "    return len(jsonLine['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_mentions(jsonLine):\n",
    "    try:\n",
    "        mentions=len(jsonLine['mentions'])\n",
    "        return mentions\n",
    "    except:\n",
    "        mentions=len(jsonLine['user_mentions'])\n",
    "        return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_urls(jsonLine):\n",
    "    return len(jsonLine['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_emoticons(jsonLine):\n",
    "    try:\n",
    "        emoticons= len(jsonLine['emoticons'])\n",
    "        return emoticons\n",
    "    except:\n",
    "        emoticons=len(jsonLine['symbols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swear_count(jsonLine):\n",
    "    swearCount=0\n",
    "    swears=['broch','cabrões','cabr','cagalhão','caralh','cona','corn','enrabar','cuzinho','esporra','fod','mamadas','panasca','peid','piça','picha','pila','tomates','puta','merd','caralho','crl','fds','fdç','paneleir']\n",
    "    for swear in swears:\n",
    "        if swear.lower() in jsonLine.lower():\n",
    "            swearCount+=1\n",
    "            \n",
    "    return swearCount        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_favorites_count(jsonLine):\n",
    "    return jsonLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_tweet(jsonLine):\n",
    "    dateStr=str(jsonLine)\n",
    "       \n",
    "    fullDateArray=dateStr.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_count(jsonLine):\n",
    "    sentence=jsonLine\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    verbCount=0\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            verbCount+=1\n",
    "        \n",
    "    return verbCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_count(jsonLine):\n",
    "    sentence=jsonLine\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    nounCount=0\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_=='NOUN' or token.pos=='PROPN':\n",
    "            nounCount+=1\n",
    "        \n",
    "    return nounCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_count(jsonLine):\n",
    "    sentence=jsonLine\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    adjCount=0\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_=='ADJ':\n",
    "            adjCount+=1\n",
    "        \n",
    "    return adjCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_length(jsonLine):\n",
    "    return len(jsonLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataFrame(tweets):\n",
    "   # columns=['Username','Text','Time of Comment','Number of Stopwords','Number of Hashtags','Number of Mentions','Number of Urls',\n",
    "            # 'Number of Emoticons','Number of Swears','Number of Favorites','Number of Verbs','Number of Nouns','Number of Adjectives','Tweet Length']\n",
    "   \n",
    "\n",
    "    #df= pd.DataFrame.from_records([tweets.to_dict() for t in tweets])\n",
    "    \n",
    "    variables = tweets[0].to_dict().keys()\n",
    "    df = pd.DataFrame([[getattr(i,j) for j in variables] for i in tweets], columns = variables)\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    return df.to_csv('firstcsv.csv',sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= pd.read_csv('C:\\\\Users\\\\tiago\\\\Desktop\\\\tese\\\\codigo\\\\pythonnotebooks\\\\repo\\\\firstcsv',error_bad_lines=False)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''print(json['text'][i])\n",
    "            print(\"Time of Comment: \" + str(get_time_of_comment(json['created_at'][i])))\n",
    "            print(\"Number of Stopwords: \"+ str(get_stopword_nr(json['text'][i])))\n",
    "            print(\"Number of Hashtags: \"+ str(get_nr_of_hashtags(json['metadata'][i])))\n",
    "            print(\"Number of Mentions: \"+ str(get_nr_of_mentions(json['metadata'][i])))\n",
    "            print(\"Number of Urls: \"+ str(get_nr_of_urls(json['metadata'][i])))\n",
    "            print(\"Number of Emoticons: \"+ str(get_nr_of_emoticons(json['metadata'][i])))\n",
    "            print(\"Number of Swears: \"+ str(get_swear_count(json['text'][i])))\n",
    "            print(\"Number of Favorites: \"+ str(get_favorites_count(json['favorite_count'][i])))\n",
    "            print(\"Number of Verbs: \"+ str(get_verb_count(json['text'][i])))\n",
    "            print(\"Number of Nouns: \"+ str(get_noun_count(json['text'][i])))\n",
    "            print(\"Number of Adjectives: \"+ str(get_adj_count(json['text'][i])))\n",
    "            print(\"Tweet Length \"+ str(get_tweet_length(json['text'][i])))\n",
    "              \n",
    "            print(\"\\n -------------------------------------------------------\\n\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "      \n",
    "        \n",
    "        def __init__(self, username,text, time_of_comment, nrStopwords, nrHashtags, nrMentions, nrUrls, nrEmoticons, nrSwears, nrFavs,\n",
    "                  nrVerbs,nrNouns,nrAdjectives,tweetLength):\n",
    "            self.username=username\n",
    "            self.text=text\n",
    "            self.time_of_comment=time_of_comment\n",
    "            self.nrStopwords=nrStopwords\n",
    "            self.nrHashtags=nrHashtags\n",
    "            self.nrMentions=nrMentions\n",
    "            self.nrUrls=nrUrls\n",
    "            self.nrEmoticons=nrEmoticons\n",
    "            self.nrSwears=nrSwears\n",
    "            self.nrFavs=nrFavs\n",
    "            self.nrVerbs=nrVerbs\n",
    "            self.nrNouns=nrNouns\n",
    "            self.nrAdjectives=nrAdjectives\n",
    "            self.tweetLength=tweetLength\n",
    "            \n",
    "            \n",
    "        def __str__(self):\n",
    "            return self.username + \" \" + self.text +\" \" +self.time_of_comment\n",
    "        \n",
    "        def to_dict(self):\n",
    "            return {\n",
    "            'username':self.username,\n",
    "            'text':self.text,\n",
    "            'time_of_comment': self.time_of_comment,\n",
    "            'tweetLength':self.tweetLength,\n",
    "            'nrStopwords': self.nrStopwords,\n",
    "            'nrHashtags':self.nrHashtags,\n",
    "            'nrMentions':self.nrMentions,\n",
    "            'nrUrls':self.nrUrls,\n",
    "            'nrEmoticons':self.nrEmoticons,\n",
    "            'nrSwears':self.nrSwears,\n",
    "            'nrFavs':self.nrFavs,\n",
    "            'nrVerbs':self.nrVerbs,\n",
    "            'nrNouns':self.nrNouns,\n",
    "            'nrAdjectives':self.nrAdjectives\n",
    "            }\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
