{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 json files found\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import spacy\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "\n",
    "\n",
    "jsons=[]\n",
    "for filename in glob.iglob(\"D:\\\\socialbus\\\\users\\\\*\" + '*', recursive=True):\n",
    "    try:\n",
    "        jsons.append(pd.read_json(filename,lines=True))\n",
    "        #print(filename)\n",
    "        #print(\"deu\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(str(len(jsons)) + \" json files found\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @eujafizaquiloou: Filipinos brancos ou negros\n",
      "\n",
      "Negros-fav\n",
      "\n",
      "Brancos-Rt\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-13f4802ff1b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m               \u001b[0mget_noun_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_adj_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m               get_tweet_length(json['text'][i]),get_allcaps_to_word_ratio(json['text'][i]))\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_allcaps_to_word_ratio' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-13f4802ff1b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             t1=Tweet(json['user'][i]['screen_name'],json['text'][i],get_sentiment_polarity(json['text'][i]),get_time_of_comment(json['created_at'][i]),\n\u001b[0m\u001b[0;32m     26\u001b[0m               \u001b[0mget_stopword_nr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_nr_of_hashtags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m               \u001b[0mget_nr_of_mentions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mget_nr_of_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-0de952e72015>\u001b[0m in \u001b[0;36mget_sentiment_polarity\u001b[1;34m(jsonLine)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_sentiment_polarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjsonLine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msentilex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentimentFlex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiLexFlexToDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# dict to use to know the sentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0msentimentValue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentimentFlex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentilex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjsonLine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\tese\\codigo\\pythonnotebooks\\repo\\sentimentFlex.py\u001b[0m in \u001b[0;36msentiLexFlexToDict\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[0minputFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mexpressions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputFile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0msplitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mDicionarioAux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda\\envs\\myEnv\\lib\\codecs.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m         \u001b[1;34m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda\\envs\\myEnv\\lib\\codecs.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;34m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda\\envs\\myEnv\\lib\\codecs.py\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size, keepends)\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;31m# If size is given, we call read() only once\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirstline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m                 \u001b[1;31m# If we're at a \"\\r\" read one extra character (which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda\\envs\\myEnv\\lib\\codecs.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size, chars, firstline)\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[0mnewdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m                 \u001b[0mnewdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[1;31m# decode bytes (those remaining from the last call included)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbytebuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    i=0\n",
    "    tweets=[]\n",
    "    \n",
    "    for json in jsons:\n",
    "        i=len(json)-1\n",
    "        while(i>1):\n",
    "        \n",
    "            try:\n",
    "          \n",
    "                t1=Tweet(json['user'][i]['screen_name'],json['text'][i],get_sentiment_polarity(json['text'][i]),get_time_of_comment(json['created_at'][i]),\n",
    "                  get_stopword_nr(json['text'][i]),get_nr_of_hashtags(json['metadata'][i]),\n",
    "                    get_nr_of_mentions(json['metadata'][i]),get_nr_of_urls(json['metadata'][i]),\n",
    "                  get_nr_of_emoticons(json['metadata'][i]),get_swear_count(json['text'][i]),\n",
    "                  get_favorites_count(json['favorite_count'][i]),get_verb_count(json['text'][i]),\n",
    "                  get_noun_count(json['text'][i]),get_adj_count(json['text'][i]),\n",
    "                  get_tweet_length(json['text'][i]),get_allcaps_to_word_ratio(json['text'][i]))\n",
    "            \n",
    "                  \n",
    "        \n",
    "                tweets.append(t1)\n",
    "                break\n",
    "            \n",
    "            except:\n",
    "        \n",
    "                t1=Tweet(json['user'][i]['screen_name'],json['text'][i],get_sentiment_polarity(json['text'][i]),get_time_of_comment(json['created_at'][i]),\n",
    "                  get_stopword_nr(json['text'][i]),get_nr_of_hashtags(json['entities'][i]),\n",
    "                  get_nr_of_mentions(json['entities'][i]),get_nr_of_urls(json['entities'][i]),\n",
    "                  get_nr_of_emoticons(json['entities'][i]),get_swear_count(json['text'][i]),\n",
    "                  get_favorites_count(json['favorite_count'][i]),get_verb_count(json['text'][i]),\n",
    "                  get_noun_count(json['text'][i]),get_adj_count(json['text'][i]),\n",
    "                  get_tweet_length(json['text'][i]),get_allcaps_to_word_ratio(json['text'][i]))\n",
    "                \n",
    "        \n",
    "                tweets.append(t1)\n",
    "                break\n",
    "            \n",
    "        \n",
    "            i=i-1 \n",
    "           \n",
    "\n",
    "    \n",
    "    #data=pd.Series(tweets)\n",
    "    create_dataFrame(tweets)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_nr(sentence):\n",
    "    \n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    count=0\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    for stopword in stopwords:\n",
    "        for word in word_tokens:\n",
    "            if stopword==word:\n",
    "                count+=1\n",
    "       \n",
    "    \n",
    "    return count\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_of_comment(jsonLine):\n",
    "     \n",
    "    dateStr=str(jsonLine)\n",
    "       \n",
    "    fullDateArray=dateStr.split()\n",
    "    time=fullDateArray[1].split('+')\n",
    "    \n",
    "    return time[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_hashtags(jsonLine):\n",
    "    return len(jsonLine['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_mentions(jsonLine):\n",
    "    try:\n",
    "        mentions=len(jsonLine['mentions'])\n",
    "        return mentions\n",
    "    except:\n",
    "        mentions=len(jsonLine['user_mentions'])\n",
    "        return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_urls(jsonLine):\n",
    "    return len(jsonLine['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_emoticons(jsonLine):\n",
    "    try:\n",
    "        emoticons= len(jsonLine['emoticons'])\n",
    "        return emoticons\n",
    "    except:\n",
    "        emoticons=len(jsonLine['symbols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swear_count(jsonLine):\n",
    "    swearCount=0\n",
    "    swears=['broch','cabrões','cabr','cagalhão','caralh','cona','corn','enrabar','cuzinho','esporra','fod','mamadas','panasca','peid','piça','picha','pila','tomates','puta','merd','caralho','crl','fds','fdç','paneleir']\n",
    "    for swear in swears:\n",
    "        if swear.lower() in jsonLine.lower():\n",
    "            swearCount+=1\n",
    "            \n",
    "    return swearCount        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_favorites_count(jsonLine):\n",
    "    return jsonLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_tweet(jsonLine):\n",
    "    dateStr=str(jsonLine)\n",
    "       \n",
    "    fullDateArray=dateStr.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_count(jsonLine):\n",
    "    sentence=jsonLine\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    verbCount=0\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            verbCount+=1\n",
    "        \n",
    "    return verbCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_count(jsonLine):\n",
    "    sentence=jsonLine\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    nounCount=0\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_=='NOUN' or token.pos=='PROPN':\n",
    "            nounCount+=1\n",
    "        \n",
    "    return nounCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_count(jsonLine):\n",
    "    sentence=jsonLine\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    adjCount=0\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_=='ADJ':\n",
    "            adjCount+=1\n",
    "        \n",
    "    return adjCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_length(jsonLine):\n",
    "    return len(jsonLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentimentFlex\n",
    "\n",
    "def get_sentiment_polarity(jsonLine):\n",
    "  \n",
    "    sentilex = sentimentFlex.sentiLexFlexToDict() # dict to use to know the sentiment\n",
    "    sentimentValue = sentimentFlex.polarity(sentilex,jsonLine)\n",
    "    \n",
    " \n",
    "    return sentimentValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataFrame(tweets):\n",
    "   # columns=['Username','Text','Time of Comment','Number of Stopwords','Number of Hashtags','Number of Mentions','Number of Urls',\n",
    "            # 'Number of Emoticons','Number of Swears','Number of Favorites','Number of Verbs','Number of Nouns','Number of Adjectives','Tweet Length']\n",
    "   \n",
    "\n",
    "    #df= pd.DataFrame.from_records([tweets.to_dict() for t in tweets])\n",
    "    \n",
    "    variables = tweets[0].to_dict().keys()\n",
    "    df = pd.DataFrame([[getattr(i,j) for j in variables] for i in tweets], columns = variables)\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    return df.to_csv('firstcsv.csv',sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_allcaps_to_word_ratio(jsonLine):\n",
    "  \n",
    "   \n",
    "    sentence=jsonLine.split()\n",
    "    matches = re.findall(\"([A-Z]+\\s?[A-Z]+[^a-z0-9\\W])\",jsonLine)\n",
    "\n",
    "    if(sentence>0):\n",
    "        ratio= len(matches)/len(sentence)\n",
    "        return ratio\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "      \n",
    "        \n",
    "        def __init__(self, username,text,sentimentPolarity, time_of_comment, nrStopwords, nrHashtags, nrMentions, nrUrls, nrEmoticons, nrSwears, nrFavs,\n",
    "                  nrVerbs,nrNouns,nrAdjectives,tweetLength,allCapsToWordRatio):\n",
    "            self.username=username\n",
    "            self.text=text\n",
    "            self.sentimentPolarity=sentimentPolarity\n",
    "            self.time_of_comment=time_of_comment\n",
    "            self.nrStopwords=nrStopwords\n",
    "            self.nrHashtags=nrHashtags\n",
    "            self.nrMentions=nrMentions\n",
    "            self.nrUrls=nrUrls\n",
    "            self.nrEmoticons=nrEmoticons\n",
    "            self.nrSwears=nrSwears\n",
    "            self.nrFavs=nrFavs\n",
    "            self.nrVerbs=nrVerbs\n",
    "            self.nrNouns=nrNouns\n",
    "            self.nrAdjectives=nrAdjectives\n",
    "            self.tweetLength=tweetLength\n",
    "            self.allCapsToWordRatio=allCapsToWordRatio\n",
    "            \n",
    "            \n",
    "            \n",
    "        def __str__(self):\n",
    "            return self.username + \" \" + self.text +\" \" +self.time_of_comment\n",
    "        \n",
    "        def to_dict(self):\n",
    "            return {\n",
    "            'username':self.username,\n",
    "            'text':self.text,\n",
    "            'time_of_comment': self.time_of_comment,\n",
    "            'sentimentPolarity': self.sentimentPolarity,\n",
    "            'tweetLength':self.tweetLength,\n",
    "            'nrStopwords': self.nrStopwords,\n",
    "            'nrHashtags':self.nrHashtags,\n",
    "            'nrMentions':self.nrMentions,\n",
    "            'nrUrls':self.nrUrls,\n",
    "            'nrEmoticons':self.nrEmoticons,\n",
    "            'nrSwears':self.nrSwears,\n",
    "            'nrFavs':self.nrFavs,\n",
    "            'nrVerbs':self.nrVerbs,\n",
    "            'nrNouns':self.nrNouns,\n",
    "            'nrAdjectives':self.nrAdjectives,\n",
    "            'allCapsToWordRatio':self.allCapsToWordRatio\n",
    "            }\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
