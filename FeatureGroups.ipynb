{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 json files found\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import spacy\n",
    "nlp = spacy.load('pt')\n",
    "\n",
    "\n",
    "\n",
    "jsons=[]\n",
    "for filename in glob.iglob(\"D:\\\\socialbus\\\\users\\\\*\" + '*', recursive=True):\n",
    "    try:\n",
    "        jsons.append(pd.read_json(filename,lines=True))\n",
    "        #print(filename)\n",
    "        #print(\"deu\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(str(len(jsons)) + \" json files found\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "d=[]\n",
    "j=0\n",
    "\n",
    "for json in jsons:\n",
    "    i=len(json)-1\n",
    "    j+=1\n",
    "    while(i>1):\n",
    "        \n",
    "        try:\n",
    "          \n",
    "            \n",
    "            d.append({'Username': json['user'][i]['screen_name'],'Text':json['text'][i],'Time of Comment': get_time_of_comment(json['created_at'][i]),\n",
    "                  'Number of Stopwords':get_stopword_nr(json['text'][i]),\"Number of Hashtags\":get_nr_of_hashtags(json['metadata'][i]),\n",
    "                  \"Number of Mentions\":get_nr_of_mentions(json['metadata'][i]),\"Number of Urls\":get_nr_of_urls(json['metadata'][i]),\n",
    "                  \"Number of Emoticons\":get_nr_of_emoticons(json['metadata'][i]),\"Number of Swears\":get_swear_count(json['text'][i]),\n",
    "                  \"Number of Favorites\":get_favorites_count(json['favorite_count'][i]),\"Number of Verbs\":get_verb_count(json['text'][i]),\n",
    "                  \"Number of Nouns\" :get_noun_count(json['text'][i]),\"Number of Adjectives\":get_adj_count(json['text'][i]),\n",
    "                  \"Tweet Length\":get_tweet_length(json['text'][i])})\n",
    "        \n",
    "        \n",
    "        except:\n",
    "        \n",
    "             d.append({'Username': json['user'][i]['screen_name'],'Text':json['text'][i],'Time of Comment': get_time_of_comment(json['created_at'][i]),\n",
    "                  'Number of Stopwords':get_stopword_nr(json['text'][i]),\"Number of Hashtags\":get_nr_of_hashtags(json['entities'][i]),\n",
    "                  \"Number of Mentions\":get_nr_of_mentions(json['entities'][i]),\"Number of Urls\":get_nr_of_urls(json['entities'][i]),\n",
    "                  \"Number of Emoticons\":get_nr_of_emoticons(json['entities'][i]),\"Number of Swears\":get_swear_count(json['text'][i]),\n",
    "                  \"Number of Favorites\":get_favorites_count(json['favorite_count'][i]),\"Number of Verbs\":get_verb_count(json['text'][i]),\n",
    "                  \"Number of Nouns\" :get_noun_count(json['text'][i]),\"Number of Adjectives\":get_adj_count(json['text'][i]),\n",
    "                  \"Tweet Length\":get_tweet_length(json['text'][i])})\n",
    "        \n",
    "           \n",
    "\n",
    "        \n",
    "        i=i-1\n",
    "   \n",
    "\n",
    "create_dataframe(d)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_nr(sentence):\n",
    "    \n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    count=0\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    for stopword in stopwords:\n",
    "        for word in word_tokens:\n",
    "            if stopword==word:\n",
    "                count+=1\n",
    "       \n",
    "    \n",
    "    return count\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_of_comment(jsonLine):\n",
    "     \n",
    "    dateStr=str(jsonLine)\n",
    "       \n",
    "    fullDateArray=dateStr.split()\n",
    "    time=fullDateArray[1].split('+')\n",
    "    \n",
    "    return time[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_hashtags(jsonLine):\n",
    "    return len(jsonLine['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_mentions(jsonLine):\n",
    "    try:\n",
    "        mentions=len(jsonLine['mentions'])\n",
    "        return mentions\n",
    "    except:\n",
    "        mentions=len(jsonLine['user_mentions'])\n",
    "        return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_urls(jsonLine):\n",
    "    return len(jsonLine['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_of_emoticons(jsonLine):\n",
    "    try:\n",
    "        emoticons= len(jsonLine['emoticons'])\n",
    "        return emoticons\n",
    "    except:\n",
    "        emoticons=len(jsonLine['symbols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swear_count(jsonLine):\n",
    "    swearCount=0\n",
    "    swears=['broch','cabrões','cabr','cagalhão','caralh','cona','corn','enrabar','cuzinho','esporra','fod','mamadas','panasca','peid','piça','picha','pila','tomates','puta','merd','caralho','crl','fds','fdç','paneleir']\n",
    "    for swear in swears:\n",
    "        if swear.lower() in jsonLine.lower():\n",
    "            swearCount+=1\n",
    "            \n",
    "    return swearCount        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_favorites_count(jsonLine):\n",
    "    return jsonLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_tweet(jsonLine):\n",
    "    dateStr=str(jsonLine)\n",
    "       \n",
    "    fullDateArray=dateStr.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_count(jsonLine):\n",
    "    sentence=jsonLine\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    verbCount=0\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_=='VERB':\n",
    "            verbCount+=1\n",
    "        \n",
    "    return verbCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_count(jsonLine):\n",
    "    sentence=jsonLine\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    nounCount=0\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_=='NOUN' or token.pos=='PROPN':\n",
    "            nounCount+=1\n",
    "        \n",
    "    return nounCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_count(jsonLine):\n",
    "    sentence=jsonLine\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    adjCount=0\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_=='ADJ':\n",
    "            adjCount+=1\n",
    "        \n",
    "    return adjCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_length(jsonLine):\n",
    "    return len(jsonLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataFrame(d):\n",
    "    \n",
    "    for p in game.players.passing():\n",
    "        d.append({'Player': p, 'Team': p.team, 'Passer Rating':\n",
    "            p.passer_rating()})\n",
    "\n",
    "    pd.DataFrame(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''print(json['text'][i])\n",
    "            print(\"Time of Comment: \" + str(get_time_of_comment(json['created_at'][i])))\n",
    "            print(\"Number of Stopwords: \"+ str(get_stopword_nr(json['text'][i])))\n",
    "            print(\"Number of Hashtags: \"+ str(get_nr_of_hashtags(json['metadata'][i])))\n",
    "            print(\"Number of Mentions: \"+ str(get_nr_of_mentions(json['metadata'][i])))\n",
    "            print(\"Number of Urls: \"+ str(get_nr_of_urls(json['metadata'][i])))\n",
    "            print(\"Number of Emoticons: \"+ str(get_nr_of_emoticons(json['metadata'][i])))\n",
    "            print(\"Number of Swears: \"+ str(get_swear_count(json['text'][i])))\n",
    "            print(\"Number of Favorites: \"+ str(get_favorites_count(json['favorite_count'][i])))\n",
    "            print(\"Number of Verbs: \"+ str(get_verb_count(json['text'][i])))\n",
    "            print(\"Number of Nouns: \"+ str(get_noun_count(json['text'][i])))\n",
    "            print(\"Number of Adjectives: \"+ str(get_adj_count(json['text'][i])))\n",
    "            print(\"Tweet Length \"+ str(get_tweet_length(json['text'][i])))\n",
    "              \n",
    "            print(\"\\n -------------------------------------------------------\\n\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
